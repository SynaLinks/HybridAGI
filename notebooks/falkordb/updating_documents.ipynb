{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updating the Document Memory (on the fly)\n",
    "\n",
    "To be able to learn our Agent system needs to be able to modify its memory on the fly. This feature particularly important in any learning system is often not present in Agent systems. Because actually integrating new documents or facts into memory include also the data processing pipeline needed to chunk/extract and embed the information.\n",
    "\n",
    "With HybridAGI, we have a very elegant way of doing it by using on the fly data processing pipelines. Allowing the Agent to learn on the fly new knowledge. However this technique should not replace the traditional way of integrating prior knowledge to the system like presented in other notebooks, that is two different kind of applications. This way of populating the memory should be only used when working with an Agent system that needs to learn, or if you are building a scrapper Agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hybridagi.core.graph_program as gp\n",
    "\n",
    "main = gp.GraphProgram(\n",
    "    name = \"main\",\n",
    "    description = \"The main program\",\n",
    ")\n",
    "\n",
    "main.add(gp.Action(\n",
    "    id = \"search_docs\",\n",
    "    purpose = \"Search for relevant documents to answer the Objective's question\",\n",
    "    tool = \"DocumentSearch\",\n",
    "    prompt = \"Please use the Objective's question to infer the search query\",\n",
    "))\n",
    "\n",
    "main.add(gp.Decision(\n",
    "    id = \"check_context\",\n",
    "    purpose = \"Check if the answer to the Objective's question is in your Context\",\n",
    "    question = \"Is the answer to the Objective's question in your context?\"\n",
    "))\n",
    "\n",
    "main.add(gp.Action(\n",
    "    id = \"answer_context_based\",\n",
    "    purpose = \"Answer to the Objective's question based on your Context\",\n",
    "    tool = \"Speak\",\n",
    "    prompt = \"Answer to the Objective's question, if there is relevant information in your Context, please use it\",\n",
    "))\n",
    "\n",
    "main.add(gp.Action(\n",
    "    id = \"answer\",\n",
    "    purpose = \"Answer to the Objective's question\",\n",
    "    tool = \"Speak\",\n",
    "    prompt = \"Answer to the Objective's question, don't say it is based on your search just answer\",\n",
    "))\n",
    "\n",
    "main.add(gp.Action(\n",
    "    id = \"save_answer\",\n",
    "    purpose = \"Save the answer to the Objective question into memory\",\n",
    "    tool = \"AddDocument\",\n",
    "    prompt = \"Use the answer in your context to infer the document to save, never explain what you are doing\",\n",
    "))\n",
    "\n",
    "main.connect(\"start\", \"search_docs\")\n",
    "main.connect(\"search_docs\", \"check_context\")\n",
    "main.connect(\"check_context\", \"answer_context_based\", label=\"Yes\")\n",
    "main.connect(\"check_context\", \"answer\", label=\"No\")\n",
    "main.connect(\"answer\", \"save_answer\")\n",
    "main.connect(\"answer_context_based\", \"end\")\n",
    "main.connect(\"save_answer\", \"end\")\n",
    "\n",
    "main.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add the programs into memory\n",
    "from hybridagi.memory.integration.falkordb import FalkorDBProgramMemory, FalkorDBDocumentMemory\n",
    "from hybridagi.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "\n",
    "embeddings = SentenceTransformerEmbeddings(\n",
    "    model_name_or_path = \"all-MiniLM-L6-v2\",\n",
    "    dim = 384,\n",
    ")\n",
    "\n",
    "program_memory = FalkorDBProgramMemory(\n",
    "    index_name=\"update_memory\",\n",
    "    embeddings=embeddings,\n",
    "    wipe_on_start=True,\n",
    ")\n",
    "\n",
    "program_memory.update(main)\n",
    "\n",
    "# Then we instanciate the document memory for later use\n",
    "document_memory = FalkorDBDocumentMemory(\n",
    "    index_name=\"update_memory\",\n",
    "    embeddings=embeddings,\n",
    "    wipe_on_start=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we setup the agent and tools\n",
    "import dspy\n",
    "from hybridagi.core.datatypes import AgentState\n",
    "from hybridagi.core.pipeline import Pipeline\n",
    "from hybridagi.embeddings import SentenceTransformerEmbeddings\n",
    "from hybridagi.modules.agents import GraphInterpreterAgent\n",
    "from hybridagi.modules.splitters import DocumentSentenceSplitter\n",
    "from hybridagi.modules.embedders import DocumentEmbedder\n",
    "from hybridagi.modules.retrievers.integration.local import FAISSDocumentRetriever \n",
    "from hybridagi.modules.agents.tools import (\n",
    "    SpeakTool,\n",
    "    DocumentSearchTool,\n",
    "    AddDocumentTool,\n",
    ")\n",
    "\n",
    "embeddings = SentenceTransformerEmbeddings(\n",
    "    model_name_or_path = \"all-MiniLM-L6-v2\",\n",
    "    dim = 384,\n",
    ")\n",
    "\n",
    "pipeline = Pipeline()\n",
    "\n",
    "pipeline.add(\"chunk_documents\", DocumentSentenceSplitter(\n",
    "    method = \"word\",\n",
    "    chunk_size = 1024,\n",
    "    chunk_overlap = 0,\n",
    "))\n",
    "pipeline.add(\"embed_chunks\", DocumentEmbedder(embeddings=embeddings))\n",
    "\n",
    "agent_state = AgentState()\n",
    "\n",
    "tools = [\n",
    "    SpeakTool(\n",
    "        agent_state = agent_state,\n",
    "    ),\n",
    "    DocumentSearchTool(\n",
    "        retriever = FAISSDocumentRetriever(\n",
    "            document_memory = document_memory,\n",
    "            embeddings = embeddings,\n",
    "            distance = \"cosine\",\n",
    "            max_distance = 1.0,\n",
    "            k = 5,\n",
    "            reranker = None,\n",
    "        ),\n",
    "    ),\n",
    "    AddDocumentTool(\n",
    "        document_memory = document_memory,\n",
    "        pipeline = pipeline # Here we bind the document processing pipeline to our tool\n",
    "    ),\n",
    "]\n",
    "\n",
    "lm = dspy.OllamaLocal(model='mistral', max_tokens=1024, stop=[\"\\n\\n\\n\"])\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "agent = GraphInterpreterAgent(\n",
    "    program_memory = program_memory,\n",
    "    agent_state = agent_state,\n",
    "    tools = tools,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hybridagi.core.datatypes import Query\n",
    "\n",
    "# Now we can test our system\n",
    "result = agent(Query(query=\"What the definition of a neuro symbolic AI\"))\n",
    "\n",
    "print(result.final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok so now we can actually try to run the agent again to see if the memory got updated correctly\n",
    "\n",
    "result = agent(Query(query=\"What the definition of a neuro symbolic AI\"))\n",
    "\n",
    "print(result.final_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hybridagi-B1GoJrSC-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
