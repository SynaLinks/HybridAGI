{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updating facts (on the fly)\n",
    "\n",
    "This notebook is going to show the same process than the `updating_documents` notebook, but with facts instead. Nothing fancy, just to show you how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hybridagi.core.graph_program as gp\n",
    "\n",
    "main = gp.GraphProgram(\n",
    "    name = \"main\",\n",
    "    description = \"The main program\",\n",
    ")\n",
    "\n",
    "main.add(gp.Action(\n",
    "    id = \"search_facts\",\n",
    "    purpose = \"Search for relevant facts to answer the Objective's question\",\n",
    "    tool = \"FactSearch\",\n",
    "    prompt = \"Please use the Objective's question to infer the search query\",\n",
    "))\n",
    "\n",
    "main.add(gp.Decision(\n",
    "    id = \"check_context\",\n",
    "    purpose = \"Check if the answer to the Objective's question is in your Context\",\n",
    "    question = \"Is the answer to the Objective's question in your context?\"\n",
    "))\n",
    "\n",
    "main.add(gp.Action(\n",
    "    id = \"answer_context_based\",\n",
    "    purpose = \"Answer to the Objective's question based on your Context\",\n",
    "    tool = \"Speak\",\n",
    "    prompt = \"Answer to the Objective's question, if there is relevant information in your Context, please use it\",\n",
    "))\n",
    "\n",
    "main.add(gp.Action(\n",
    "    id = \"answer\",\n",
    "    purpose = \"Answer to the Objective's question\",\n",
    "    tool = \"Speak\",\n",
    "    prompt = \"Answer to the Objective's question, don't say it is based on your search just answer\",\n",
    "))\n",
    "\n",
    "main.add(gp.Action(\n",
    "    id = \"save_answer\",\n",
    "    purpose = \"Save the answer to the Objective Question into memory\",\n",
    "    tool = \"AddFact\",\n",
    "    prompt = \"Use the answer in your context to infer the document to save, never explain what you are doing\",\n",
    "))\n",
    "\n",
    "main.connect(\"start\", \"search_facts\")\n",
    "main.connect(\"search_facts\", \"check_context\")\n",
    "main.connect(\"check_context\", \"answer_context_based\", label=\"Yes\")\n",
    "main.connect(\"check_context\", \"answer\", label=\"No\")\n",
    "main.connect(\"answer\", \"save_answer\")\n",
    "main.connect(\"answer_context_based\", \"end\")\n",
    "main.connect(\"save_answer\", \"end\")\n",
    "\n",
    "main.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add the programs into memory\n",
    "from hybridagi.memory.integration.falkordb import FalkorDBProgramMemory, FalkorDBFactMemory\n",
    "from hybridagi.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "\n",
    "embeddings = SentenceTransformerEmbeddings(\n",
    "    model_name_or_path = \"all-MiniLM-L6-v2\",\n",
    "    dim = 384,\n",
    ")\n",
    "\n",
    "program_memory = FalkorDBProgramMemory(\n",
    "    index_name=\"update_facts\",\n",
    "    embeddings = embeddings,\n",
    "    wipe_on_start=True,\n",
    ")\n",
    "\n",
    "program_memory.update(main)\n",
    "\n",
    "# Then we instantiate the document memory for later use\n",
    "\n",
    "fact_memory = FalkorDBFactMemory(\n",
    "    index_name=\"update_facts\",\n",
    "    embeddings = embeddings,\n",
    "    wipe_on_start=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we setup the agent and tools\n",
    "import dspy\n",
    "from hybridagi.core.datatypes import AgentState\n",
    "from hybridagi.core.pipeline import Pipeline\n",
    "from hybridagi.embeddings import SentenceTransformerEmbeddings\n",
    "from hybridagi.modules.agents import GraphInterpreterAgent\n",
    "from hybridagi.modules.splitters import DocumentSentenceSplitter\n",
    "from hybridagi.modules.embedders import FactEmbedder\n",
    "from hybridagi.modules.extractors import LLMFactExtractor\n",
    "from hybridagi.modules.deduplicators import EntityDeduplicator\n",
    "from hybridagi.modules.retrievers.integration.local import FAISSFactRetriever \n",
    "from hybridagi.modules.agents.tools import (\n",
    "    SpeakTool,\n",
    "    FactSearchTool,\n",
    "    AddFactTool,\n",
    ")\n",
    "\n",
    "embeddings = SentenceTransformerEmbeddings(\n",
    "    model_name_or_path = \"all-MiniLM-L6-v2\",\n",
    "    dim = 384,\n",
    ")\n",
    "\n",
    "pipeline = Pipeline()\n",
    "\n",
    "# The facts processing pipeline, takes an input document and convert it to facts\n",
    "\n",
    "pipeline.add(\"chunk_docs\", DocumentSentenceSplitter(\n",
    "    method = \"word\",\n",
    "    chunk_size = 100,\n",
    "    chunk_overlap = 0,\n",
    "    separator = \" \",\n",
    "))\n",
    "pipeline.add(\"extract_facts\", LLMFactExtractor())\n",
    "pipeline.add(\"deduplicate_entities\", EntityDeduplicator(method=\"exact\"))\n",
    "pipeline.add(\"embed_facts\", FactEmbedder(embeddings=embeddings))\n",
    "\n",
    "agent_state = AgentState()\n",
    "\n",
    "tools = [\n",
    "    SpeakTool(\n",
    "        agent_state = agent_state,\n",
    "    ),\n",
    "    FactSearchTool(\n",
    "        retriever = FAISSFactRetriever(\n",
    "            fact_memory = fact_memory,\n",
    "            embeddings = embeddings,\n",
    "            distance = \"cosine\",\n",
    "            max_distance = 1.0,\n",
    "            k = 5,\n",
    "            reranker = None,\n",
    "        ),\n",
    "    ),\n",
    "    AddFactTool(\n",
    "        fact_memory = fact_memory,\n",
    "        pipeline = pipeline, # Here we bind the document processing pipeline to our tool\n",
    "    ),\n",
    "]\n",
    "\n",
    "lm = dspy.OllamaLocal(model='mistral', max_tokens=1024, stop=[\"\\n\\n\\n\"])\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "agent = GraphInterpreterAgent(\n",
    "    program_memory = program_memory,\n",
    "    agent_state = agent_state,\n",
    "    tools = tools,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hybridagi.core.datatypes import Query\n",
    "\n",
    "# Now we can test our system\n",
    "result = agent(Query(query=\"What the definition of a neuro symbolic AI\"))\n",
    "\n",
    "print(result.final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hybridagi.core.datatypes import Query\n",
    "\n",
    "# Now we can test our system\n",
    "result = agent(Query(query=\"What the definition of a neuro symbolic AI\"))\n",
    "\n",
    "print(result.final_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hybridagi-B1GoJrSC-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
