{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Only RAG\n",
    "\n",
    "In this notebook we are going to explore how to make a vector only RAG system. RAG stands for Retrieval Augmented Generation, the basic idea of this technique is to not use the LLM to recall knowledge but instead rely on a external system like a database to provide information to the system.\n",
    "\n",
    "The main advantage of vector-only RAG systems over Knowledge Graph RAG is the fact that you don't need to model your knowledge domain and can use very simple data pipelines. They excel in retrieving information based on the context of the question but fail short in retrieving factual knowledge (you've been warned).\n",
    "\n",
    "In this tutorial we are going to present you a small system that is nice to learn how RAG works in practice. To make things bit more complicated, we are going to start from a PDF file.\n",
    "\n",
    "The PDF we are going to parse is a paper from [Elisabeth Spelke](https://en.wikipedia.org/wiki/Elizabeth_Spelke) a developmental psychologist that studied cognition in infants, culturally different communities and non-human species to understand the fundamental building blocks of our mind. It contains evidences that our cognition system is build upon several key components and is an important inspiration for Neuro-Symbolic and Robotics systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hybridagi.core.pipeline import Pipeline\n",
    "from hybridagi.embeddings import SentenceTransformerEmbeddings\n",
    "from hybridagi.modules.splitters import DocumentSentenceSplitter\n",
    "from hybridagi.modules.embedders import DocumentEmbedder\n",
    "from hybridagi.readers import PDFReader\n",
    "\n",
    "embeddings = SentenceTransformerEmbeddings(\n",
    "    model_name_or_path = \"all-MiniLM-L6-v2\",\n",
    "    dim = 384, # The dimention of the embeddings vector (also called dense vector)\n",
    ")\n",
    "\n",
    "reader = PDFReader()\n",
    "\n",
    "input_docs = reader(\"../data/SpelkeKinzlerCoreKnowledge.pdf\")\n",
    "\n",
    "# Now that we have our input documents, we can start to make our data processing pipeline\n",
    "\n",
    "pipeline = Pipeline()\n",
    "\n",
    "pipeline.add(\"chunk_documents\", DocumentSentenceSplitter(\n",
    "    method = \"word\",\n",
    "    chunk_size = 1024,\n",
    "    chunk_overlap = 0,\n",
    "))\n",
    "pipeline.add(\"embed_chunks\", DocumentEmbedder(embeddings=embeddings))\n",
    "\n",
    "output_docs = pipeline(input_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the documents into memory\n",
    "\n",
    "Now that we have our documents we can load them into memory, for storing unstructured textual documents, we provide the `DocumentMemory` and for this example we are going to use the local integration for rapid prototyping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hybridagi.memory.integration.falkordb import FalkorDBDocumentMemory\n",
    "\n",
    "document_memory = FalkorDBDocumentMemory(\n",
    "    index_name=\"vector_rag_agent\",\n",
    "    embeddings=embeddings,\n",
    "    wipe_on_start=True,\n",
    ")\n",
    "\n",
    "document_memory.update(input_docs)\n",
    "document_memory.update(output_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a RAG Agent\n",
    "\n",
    "Now that our data is ready and loaded into memory, we can start making our RAG Agent, but first we need to create our graph program that is going to encode the Agent behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hybridagi.core.graph_program as gp\n",
    "\n",
    "# We first need to program our RAG Agent using Graph Prompt Programs\n",
    "# Here we a simple program that involve 2 steps\n",
    "# We first retrieve documents, then answer to the objective's question based on them\n",
    "\n",
    "main = gp.GraphProgram(\n",
    "    name = \"main\",\n",
    "    description = \"The main program\",\n",
    ")\n",
    "\n",
    "main.add(gp.Action(\n",
    "    id = \"document_search\",\n",
    "    purpose = \"Find relevant documents\",\n",
    "    tool = \"DocumentSearch\",\n",
    "    prompt = \"Please infer the similarity search query (only ONE item) based on the Objective's question\",\n",
    "))\n",
    "\n",
    "main.add(gp.Action(\n",
    "    id = \"answer\",\n",
    "    purpose = \"Answer the Objective's question\",\n",
    "    tool = \"Speak\",\n",
    "    prompt = \"\"\"\n",
    "Please answer the Objective's question using the relevant documents in your context.\n",
    "If no document are relevant just say that you don't know.\n",
    "Don't state the Objective's question and only give the correct answer.\n",
    "\"\"\",\n",
    "))\n",
    "\n",
    "main.connect(\"start\", \"document_search\")\n",
    "main.connect(\"document_search\", \"answer\")\n",
    "main.connect(\"answer\", \"end\")\n",
    "\n",
    "main.build() # Verify that the graph program is correct\n",
    "\n",
    "print(main) # Let's look at it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can add this program into memory\n",
    "\n",
    "from hybridagi.memory.integration.falkordb import FalkorDBProgramMemory\n",
    "\n",
    "program_memory = FalkorDBProgramMemory(\n",
    "    index_name=\"vector_rag_agent\",\n",
    "    embeddings=embeddings,\n",
    ")\n",
    "\n",
    "program_memory.update(main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then instanciate our tools and agent system\n",
    "\n",
    "import dspy\n",
    "from hybridagi.core.datatypes import AgentState, Query\n",
    "from hybridagi.modules.agents import GraphInterpreterAgent\n",
    "from hybridagi.modules.retrievers.integration.local import FAISSDocumentRetriever\n",
    "from hybridagi.modules.agents.tools import (\n",
    "    SpeakTool,\n",
    "    DocumentSearchTool,\n",
    ")\n",
    "\n",
    "agent_state = AgentState()\n",
    "\n",
    "tools = [\n",
    "    SpeakTool(\n",
    "        agent_state = agent_state\n",
    "    ),\n",
    "    DocumentSearchTool(\n",
    "        retriever = FAISSDocumentRetriever(\n",
    "            document_memory = document_memory,\n",
    "            embeddings = embeddings,\n",
    "            distance = \"cosine\",\n",
    "            max_distance = 0.7,\n",
    "            k = 5,\n",
    "            reranker = None,\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "rag_agent = GraphInterpreterAgent(\n",
    "    program_memory = program_memory,\n",
    "    agent_state = agent_state,\n",
    "    tools = tools,\n",
    ")\n",
    "\n",
    "# We can now setup the LLM using Ollama client from DSPy\n",
    "\n",
    "lm = dspy.OllamaLocal(\n",
    "    model='mistral', \n",
    "    max_tokens=1024, \n",
    "    stop=[\"\\n\\n\\n\", \"\\n---\", \"\\n\\nContext:\"]\n",
    ")\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "# And call our agent\n",
    "\n",
    "result = rag_agent(Query(query=\"What are the core knowledge?\"))\n",
    "\n",
    "print(result.final_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hybridagi-B1GoJrSC-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
